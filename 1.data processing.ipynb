{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 数据预处理：异常值、过采样和特征排序"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#Import the Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Dataset\n",
    "Dataset = pd.read_csv(\"../code/Data/german/german.csv\")"
   ]
  },
  {
   "source": [
    "## 1.异常值处理：三倍标准差探测法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['A2','A5','A13']:\n",
    "    std = np.std(Dataset[i])\n",
    "    mean = np.mean(Dataset[i])\n",
    "    b = 3\n",
    "    lower_limit = mean-b*std\n",
    "    upper_limit = mean+b*std\n",
    "    Dataset=Dataset.drop(Dataset[(Dataset[i]>upper_limit)|(Dataset[i]<lower_limit)].index)\n",
    "X = Dataset.iloc[:, :-1]\n",
    "y = Dataset.iloc[:, [-1]]"
   ]
  },
  {
   "source": [
    "## 2.字符型数据编码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the string value and convert them in to int values!!\n",
    "#Note we are doing only labelencoder \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X =X.values\n",
    "y =y.values\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "for i in range(20):\n",
    "    if (i not in [1,4,7,10,12,15,17]):\n",
    "        X[:,i]=labelencoder.fit_transform(X[:, i])"
   ]
  },
  {
   "source": [
    "## 3.基于最近邻的SMOTE抽样"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.reshape(1,-1)\n",
    "y=pd.Series(y[0,:])\n",
    "X=pd.DataFrame(X)\n",
    "new_Dataset=pd.concat([X,y],axis=1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "neighbors NearestNeighbors()\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "class Smote:\n",
    "    def __init__(self,samples,N=10,k=5):\n",
    "        self.n_samples,self.n_attrs=samples.shape\n",
    "        self.N=N\n",
    "        self.k=k\n",
    "        self.samples=samples\n",
    "        self.newindex=0\n",
    "       # self.synthetic=np.zeros((self.n_samples*N,self.n_attrs))\n",
    "\n",
    "    def over_sampling(self):\n",
    "        N=int(self.N/100)\n",
    "        self.synthetic = np.zeros((self.n_samples * N, self.n_attrs))\n",
    "        neighbors=NearestNeighbors(n_neighbors=self.k).fit(self.samples)\n",
    "        print('neighbors',neighbors)\n",
    "        for i in range(len(self.samples)):\n",
    "            nnarray=neighbors.kneighbors(self.samples[i].reshape(1,-1),return_distance=False)[0]\n",
    "            #print nnarray\n",
    "            self._populate(N,i,nnarray)\n",
    "        return self.synthetic\n",
    "\n",
    "\n",
    "    # for each minority class samples,choose N of the k nearest neighbors and generate N synthetic samples.\n",
    "    def _populate(self,N,i,nnarray):\n",
    "        for j in range(N):\n",
    "            nn=random.randint(0,self.k-1)\n",
    "            dif=self.samples[nnarray[nn]]-self.samples[i]\n",
    "            gap=random.random()\n",
    "            self.synthetic[self.newindex]=self.samples[i]+gap*dif\n",
    "            self.newindex+=1\n",
    "\n",
    "# print(new_Dataset.iloc[(new_Dataset[20]==2).index])\n",
    "# print((new_Dataset[20]==[2]).index)\n",
    "a=new_Dataset.iloc[new_Dataset[new_Dataset[20]==2].index]\n",
    "# a=np.array([[1,2,3],[4,5,6],[2,3,1],[2,1,2],[2,3,4],[2,3,4]])\n",
    "a=np.array(a.iloc[:,:-1])\n",
    "s=pd.DataFrame(Smote(a,N=200).over_sampling())\n",
    "z=pd.Series([2]*len(s))\n",
    "s=pd.concat([pd.DataFrame(s),z],axis=1,ignore_index=True)\n",
    "all_Dataset=new_Dataset.append(s)\n",
    "all_Dataset.columns=Dataset.columns\n",
    "\n",
    "X = all_Dataset.iloc[:, :-1]\n",
    "y = all_Dataset.iloc[:, [-1]]"
   ]
  },
  {
   "source": [
    "## 4.Relief F 特征选择算法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "距离： [[9999.         4782.39992054  927.21788162 ... 7060.18597489\n",
      "   670.90278512  659.23540895]\n",
      " [4782.39992054 9999.         3855.26549021 ... 2278.03797159\n",
      "  4112.90101589 4124.51437241]\n",
      " [ 927.21788162 3855.26549021 9999.         ... 6133.09440658\n",
      "   259.61288031  271.31671957]\n",
      " ...\n",
      " [7060.18597489 2278.03797159 6133.09440658 ... 9999.\n",
      "  6390.85931442 6402.49385287]\n",
      " [ 670.90278512 4112.90101589  259.61288031 ... 6390.85931442\n",
      "  9999.           18.0988994 ]\n",
      " [ 659.23540895 4124.51437241  271.31671957 ... 6402.49385287\n",
      "    18.0988994  9999.        ]]\n",
      "shape= 1503 20\n",
      "new_features= [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "排序： [0, 18, 5, 13, 10, 17, 11, 7, 3, 2, 6, 14, 8, 9, 16, 12, 1, 15, 4, 19]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "def relief(features, labels, times):   # 传入特征矩阵，标签矩阵和随机选择的次数，因为有可能样本有很多，所以我们随机选择若干个样本来计算;这里的矩阵都是np.array\n",
    "    (n_samples, n_features) = np.shape(features)\n",
    "    delta = []\n",
    "    delta_features = []\n",
    "    delta_index = []\n",
    "    sample_distance = sap_distance(features)  # 计算每两个样本之间的距离\n",
    "    new_features = normalize(features)        # 对特征值归一化\n",
    "\n",
    "    \"\"\"\n",
    "    # 下面开始计算相关统计量，并对各个特征的相关统计量进行比较，最后返回各个特征值相关统计量从高到低的排名\n",
    "    # 这是将随机选取的样本代入计算出来的delta\n",
    "    for i in range(0, times):\n",
    "        randnum = randrange(0, n_samples, 1)        # 生成一个随机数\n",
    "        one_sample = features[randnum]        # 随机选择一个样本\n",
    "        (nearhit, nearmiss, nearhit_index, nearmiss_index) = search_near(sample_distance, labels, randnum, features)  # 找出猜中近邻和猜错近邻,nearhit为猜中近邻样本的行向量\n",
    "        delta.append(relevant_feature(nearhit_index, nearmiss_index, new_features, randnum))  # 计算相关统计量矩阵\n",
    "    delta = np.asarray(delta)\n",
    "    for j in range(0, n_features):\n",
    "        delta_features.append(np.sum(delta[:, j]))\n",
    "    midd = list(set(delta_features))\n",
    "    midd.sort(reverse=True)\n",
    "    for p in midd:\n",
    "        for q in range(0, len(delta_features)):\n",
    "            if delta_features[q] == p:\n",
    "                delta_index.append(q)\n",
    "    return delta_index\n",
    "    \"\"\"\n",
    "    # 这是将所有样本都带入计算的delta\n",
    "    for i in range(0, n_samples):\n",
    "        (nearhit, nearmiss, nearhit_index, nearmiss_index) = search_near(sample_distance, labels, i,\n",
    "                                                                         features)  # 找出猜中近邻和猜错近邻,nearhit为猜中近邻样本的行向量\n",
    "        delta.append(relevant_feature(nearhit_index, nearmiss_index, new_features, i))  # 计算相关统计量矩阵\n",
    "    delta = np.asarray(delta)\n",
    "    for j in range(0, n_features):\n",
    "        delta_features.append(np.sum(delta[:, j]))\n",
    "    midd = list(set(delta_features))\n",
    "    midd.sort(reverse=True)\n",
    "    for p in midd:\n",
    "        for q in range(0, len(delta_features)):\n",
    "            if delta_features[q] == p:\n",
    "                delta_index.append(q)\n",
    "    return delta_index\n",
    "\n",
    "\n",
    "def normalize(features):\n",
    "    (n_samples, n_features) = np.shape(features)\n",
    "    print(\"shape=\", n_samples, n_features)\n",
    "    fe_max = []\n",
    "    fe_min = []\n",
    "    n_deno = []\n",
    "    new_features = np.zeros((n_samples, n_features))\n",
    "    print(\"new_features=\", new_features)\n",
    "    for i in range(0, n_features):\n",
    "        max_index = np.argmax(features[:, i])\n",
    "        min_index = np.argmin(features[:, i])\n",
    "        fe_max.append(features[max_index, i])  # 计算每一个特征的最大值\n",
    "        fe_min.append(features[min_index, i])  # 计算每一个特征的最小值\n",
    "    n_deno = np.asarray(fe_max) - np.asarray(fe_min)  # 求出归一化的分母\n",
    "    for j in range(0, n_features):\n",
    "        for k in range(0, n_samples):\n",
    "            new_features[k, j] = (features[k, j]-fe_min[j]) / n_deno[j]  # 归一化\n",
    "    return new_features\n",
    "\n",
    "def sap_distance(features):\n",
    "    (n_samples, n_features) = np.shape(features)\n",
    "    distance = np.zeros((n_samples, n_samples))\n",
    "    for i in range(0, n_samples):\n",
    "        for j in range(0, n_samples):\n",
    "            diff_distance = features[i]-features[j]\n",
    "            if i == j:\n",
    "                distance[i, j] = 9999\n",
    "            else:\n",
    "                distance[i, j] = euclid_distance(diff_distance)  # 使用欧几里德距离定义样本之间的距离\n",
    "    print(\"距离：\",distance)\n",
    "    return distance\n",
    "\n",
    "def euclid_distance(diff_distance):\n",
    "    counter = np.power(diff_distance, 2)\n",
    "    counter = np.sum(counter)\n",
    "    counter = np.sqrt(counter)\n",
    "    return counter\n",
    "\n",
    "def search_near(sample_distance, labels, randnum, feartures):\n",
    "    (n_samples, n_features) = np.shape(feartures)\n",
    "    nearhit_list = []\n",
    "    nearmiss_list = []\n",
    "    hit_index = []\n",
    "    miss_index = []\n",
    "    for i in range(0, n_samples):\n",
    "        if labels[i] == labels[randnum]:\n",
    "            nearhit_list.append(sample_distance[i, randnum])  # 将距离放在一个列表里面\n",
    "            hit_index.append(i)                                 # 将样本标号放在另一个列表里面\n",
    "        else:\n",
    "            nearmiss_list.append(sample_distance[i, randnum])\n",
    "            miss_index.append(i)\n",
    "    nearhit_dis_index = nearhit_list.index(min(nearhit_list))   # 算出猜中近邻\n",
    "    nearhit_index = hit_index[nearhit_dis_index]                # 将猜中近邻的样本标号赋给nearhit_index\n",
    "\n",
    "    nearmiss_dis_index = nearmiss_list.index(min(nearmiss_list))\n",
    "    nearmiss_index = miss_index[nearmiss_dis_index]\n",
    "\n",
    "\n",
    "    nearhit = feartures[nearhit_index]\n",
    "    nearmiss = feartures[nearmiss_index]\n",
    "\n",
    "    return nearhit, nearmiss, nearhit_index, nearmiss_index\n",
    "\n",
    "def relevant_feature(nearhit_index, nearmiss_index, new_features, randnum):\n",
    "    diff_hit = abs(new_features[nearhit_index]-new_features[randnum])\n",
    "    diff_miss = abs(new_features[nearmiss_index]-new_features[randnum])\n",
    "    delta = -np.power(diff_hit, 2)+np.power(diff_miss, 2)\n",
    "    return delta\n",
    "\n",
    "\n",
    "\n",
    "out_features = np.array(X)\n",
    "labels = np.array(y)\n",
    "times = 2\n",
    "features_importance = relief(out_features, labels, times)\n",
    "print(\"排序：\", features_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns=[]\n",
    "for num in features_importance[:20]:# 17的效果最好\n",
    "    label='A'+str(num+1)\n",
    "    new_columns.append(label)\n",
    "new_X=X.loc[:,new_columns]\n",
    "\n",
    "data_new=pd.concat([new_X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.to_csv('../code/Data/german/data.csv')"
   ]
  }
 ]
}